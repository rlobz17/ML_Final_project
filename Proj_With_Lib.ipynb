{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[new value of  12\n",
      "---new value of  13\n",
      "-new value of  14\n",
      "-new value of  29\n",
      "---new value of  32\n",
      "---new value of  38\n",
      "---------------new value of  42\n",
      "----------------------------------------------new value of  43\n",
      "------------------------new value of  70\n",
      "-----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shota/ML_Final_project/DataProvider.py:135: RuntimeWarning: invalid value encountered in true_divide\n",
      "  spectrogram = (spectrogram - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------new value of  72\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------]\n",
      "[################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################]\n",
      "[-----]\n",
      "[#####]\n"
     ]
    }
   ],
   "source": [
    "import DataProvider\n",
    "prov = DataProvider.DataProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = list()\n",
    "y = list()\n",
    "trainVal = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "while prov.hasNext(True):\n",
    "    Xt, yt = prov.next(True)\n",
    "    for i in range(len(yt)):\n",
    "        if yt[i] == 1:\n",
    "            trainVal.append(i+1)\n",
    "            \n",
    "    X.append(Xt)\n",
    "    y.append(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((688, 1440), (688, 5))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.nan_to_num(np.matrix(X))\n",
    "y = np.matrix(y)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = list()\n",
    "y2 = list()\n",
    "actualVal = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "while prov.hasNext(False):\n",
    "    X2t, y2t = prov.next(False)\n",
    "    actualVal.append(y2t)\n",
    "    \n",
    "    k = np.zeros(5)\n",
    "    k[y2t - 1] = 1\n",
    "    \n",
    "    X2.append(X2t)\n",
    "    y2.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1440), (5, 5))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.nan_to_num(np.matrix(X2))\n",
    "y2 = np.matrix(y2)\n",
    "\n",
    "X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=64, activation='relu', input_dim = prov.returnSizeOfEverySpectogram()))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "688/688 [==============================] - 2s 2ms/step - loss: 1.7292 - accuracy: 0.1904\n",
      "Epoch 2/200\n",
      "688/688 [==============================] - 0s 223us/step - loss: 1.6209 - accuracy: 0.2645\n",
      "Epoch 3/200\n",
      "688/688 [==============================] - 0s 160us/step - loss: 1.5315 - accuracy: 0.3503\n",
      "Epoch 4/200\n",
      "688/688 [==============================] - 0s 426us/step - loss: 1.4555 - accuracy: 0.4244\n",
      "Epoch 5/200\n",
      "688/688 [==============================] - 0s 371us/step - loss: 1.3886 - accuracy: 0.4782\n",
      "Epoch 6/200\n",
      "688/688 [==============================] - 0s 381us/step - loss: 1.3253 - accuracy: 0.5189\n",
      "Epoch 7/200\n",
      "688/688 [==============================] - 0s 340us/step - loss: 1.2668 - accuracy: 0.5654\n",
      "Epoch 8/200\n",
      "688/688 [==============================] - 0s 210us/step - loss: 1.2122 - accuracy: 0.6061\n",
      "Epoch 9/200\n",
      "688/688 [==============================] - 0s 459us/step - loss: 1.1609 - accuracy: 0.6265\n",
      "Epoch 10/200\n",
      "688/688 [==============================] - 0s 207us/step - loss: 1.1127 - accuracy: 0.6424\n",
      "Epoch 11/200\n",
      "688/688 [==============================] - 0s 235us/step - loss: 1.0678 - accuracy: 0.6686\n",
      "Epoch 12/200\n",
      "688/688 [==============================] - 0s 158us/step - loss: 1.0257 - accuracy: 0.6817\n",
      "Epoch 13/200\n",
      "688/688 [==============================] - 0s 290us/step - loss: 0.9850 - accuracy: 0.6933\n",
      "Epoch 14/200\n",
      "688/688 [==============================] - 0s 283us/step - loss: 0.9471 - accuracy: 0.7006\n",
      "Epoch 15/200\n",
      "688/688 [==============================] - 0s 202us/step - loss: 0.9122 - accuracy: 0.7093\n",
      "Epoch 16/200\n",
      "688/688 [==============================] - 0s 197us/step - loss: 0.8790 - accuracy: 0.7209\n",
      "Epoch 17/200\n",
      "688/688 [==============================] - 0s 192us/step - loss: 0.8475 - accuracy: 0.7297\n",
      "Epoch 18/200\n",
      "688/688 [==============================] - 0s 203us/step - loss: 0.8173 - accuracy: 0.7456\n",
      "Epoch 19/200\n",
      "688/688 [==============================] - 0s 193us/step - loss: 0.7884 - accuracy: 0.7631\n",
      "Epoch 20/200\n",
      "688/688 [==============================] - 0s 268us/step - loss: 0.7619 - accuracy: 0.7762\n",
      "Epoch 21/200\n",
      "688/688 [==============================] - 0s 437us/step - loss: 0.7356 - accuracy: 0.7849\n",
      "Epoch 22/200\n",
      "688/688 [==============================] - 0s 228us/step - loss: 0.7108 - accuracy: 0.7951\n",
      "Epoch 23/200\n",
      "688/688 [==============================] - 0s 310us/step - loss: 0.6872 - accuracy: 0.8081\n",
      "Epoch 24/200\n",
      "688/688 [==============================] - 0s 168us/step - loss: 0.6644 - accuracy: 0.8227\n",
      "Epoch 25/200\n",
      "688/688 [==============================] - 0s 178us/step - loss: 0.6421 - accuracy: 0.8314\n",
      "Epoch 26/200\n",
      "688/688 [==============================] - 0s 645us/step - loss: 0.6213 - accuracy: 0.8358\n",
      "Epoch 27/200\n",
      "688/688 [==============================] - 0s 217us/step - loss: 0.6009 - accuracy: 0.8503\n",
      "Epoch 28/200\n",
      "688/688 [==============================] - 0s 216us/step - loss: 0.5814 - accuracy: 0.8532\n",
      "Epoch 29/200\n",
      "688/688 [==============================] - 0s 461us/step - loss: 0.5621 - accuracy: 0.8634\n",
      "Epoch 30/200\n",
      "688/688 [==============================] - 0s 411us/step - loss: 0.5441 - accuracy: 0.8677\n",
      "Epoch 31/200\n",
      "688/688 [==============================] - 0s 510us/step - loss: 0.5267 - accuracy: 0.8735\n",
      "Epoch 32/200\n",
      "688/688 [==============================] - 0s 716us/step - loss: 0.5099 - accuracy: 0.8779\n",
      "Epoch 33/200\n",
      "688/688 [==============================] - 0s 322us/step - loss: 0.4935 - accuracy: 0.8852\n",
      "Epoch 34/200\n",
      "688/688 [==============================] - 0s 201us/step - loss: 0.4785 - accuracy: 0.8910\n",
      "Epoch 35/200\n",
      "688/688 [==============================] - 0s 185us/step - loss: 0.4622 - accuracy: 0.8953\n",
      "Epoch 36/200\n",
      "688/688 [==============================] - 0s 462us/step - loss: 0.4475 - accuracy: 0.9012\n",
      "Epoch 37/200\n",
      "688/688 [==============================] - 0s 237us/step - loss: 0.4337 - accuracy: 0.9055\n",
      "Epoch 38/200\n",
      "688/688 [==============================] - 1s 830us/step - loss: 0.4193 - accuracy: 0.9099\n",
      "Epoch 39/200\n",
      "688/688 [==============================] - 0s 292us/step - loss: 0.4065 - accuracy: 0.9113\n",
      "Epoch 40/200\n",
      "688/688 [==============================] - 0s 202us/step - loss: 0.3936 - accuracy: 0.9128\n",
      "Epoch 41/200\n",
      "688/688 [==============================] - 0s 181us/step - loss: 0.3817 - accuracy: 0.9186\n",
      "Epoch 42/200\n",
      "688/688 [==============================] - 0s 291us/step - loss: 0.3697 - accuracy: 0.9186\n",
      "Epoch 43/200\n",
      "688/688 [==============================] - 0s 230us/step - loss: 0.3590 - accuracy: 0.9273\n",
      "Epoch 44/200\n",
      "688/688 [==============================] - 0s 165us/step - loss: 0.3482 - accuracy: 0.9273\n",
      "Epoch 45/200\n",
      "688/688 [==============================] - 0s 177us/step - loss: 0.3376 - accuracy: 0.9317\n",
      "Epoch 46/200\n",
      "688/688 [==============================] - 0s 176us/step - loss: 0.3275 - accuracy: 0.9360\n",
      "Epoch 47/200\n",
      "688/688 [==============================] - 0s 170us/step - loss: 0.3174 - accuracy: 0.9390\n",
      "Epoch 48/200\n",
      "688/688 [==============================] - 0s 186us/step - loss: 0.3076 - accuracy: 0.9404\n",
      "Epoch 49/200\n",
      "688/688 [==============================] - 0s 190us/step - loss: 0.2989 - accuracy: 0.9477\n",
      "Epoch 50/200\n",
      "688/688 [==============================] - 0s 164us/step - loss: 0.2900 - accuracy: 0.9462\n",
      "Epoch 51/200\n",
      "688/688 [==============================] - 0s 173us/step - loss: 0.2814 - accuracy: 0.9520\n",
      "Epoch 52/200\n",
      "688/688 [==============================] - 0s 171us/step - loss: 0.2733 - accuracy: 0.9564\n",
      "Epoch 53/200\n",
      "688/688 [==============================] - 0s 178us/step - loss: 0.2654 - accuracy: 0.9578\n",
      "Epoch 54/200\n",
      "688/688 [==============================] - 0s 171us/step - loss: 0.2583 - accuracy: 0.9564\n",
      "Epoch 55/200\n",
      "688/688 [==============================] - 0s 175us/step - loss: 0.2510 - accuracy: 0.9593\n",
      "Epoch 56/200\n",
      "688/688 [==============================] - 0s 187us/step - loss: 0.2440 - accuracy: 0.9608\n",
      "Epoch 57/200\n",
      "688/688 [==============================] - 0s 168us/step - loss: 0.2374 - accuracy: 0.9622\n",
      "Epoch 58/200\n",
      "688/688 [==============================] - 0s 173us/step - loss: 0.2309 - accuracy: 0.9637\n",
      "Epoch 59/200\n",
      "688/688 [==============================] - 0s 175us/step - loss: 0.2247 - accuracy: 0.9651\n",
      "Epoch 60/200\n",
      "688/688 [==============================] - 0s 172us/step - loss: 0.2186 - accuracy: 0.9651\n",
      "Epoch 61/200\n",
      "688/688 [==============================] - 0s 172us/step - loss: 0.2129 - accuracy: 0.9680\n",
      "Epoch 62/200\n",
      "688/688 [==============================] - 0s 171us/step - loss: 0.2072 - accuracy: 0.9680\n",
      "Epoch 63/200\n",
      "688/688 [==============================] - 0s 179us/step - loss: 0.2017 - accuracy: 0.9695\n",
      "Epoch 64/200\n",
      "688/688 [==============================] - 0s 250us/step - loss: 0.1967 - accuracy: 0.9709\n",
      "Epoch 65/200\n",
      "688/688 [==============================] - 0s 219us/step - loss: 0.1916 - accuracy: 0.9709\n",
      "Epoch 66/200\n",
      "688/688 [==============================] - 0s 226us/step - loss: 0.1868 - accuracy: 0.9724\n",
      "Epoch 67/200\n",
      "688/688 [==============================] - 0s 188us/step - loss: 0.1818 - accuracy: 0.9724\n",
      "Epoch 68/200\n",
      "688/688 [==============================] - 0s 194us/step - loss: 0.1775 - accuracy: 0.9724\n",
      "Epoch 69/200\n",
      "688/688 [==============================] - 0s 180us/step - loss: 0.1732 - accuracy: 0.9724\n",
      "Epoch 70/200\n",
      "688/688 [==============================] - 0s 176us/step - loss: 0.1692 - accuracy: 0.9738\n",
      "Epoch 71/200\n",
      "688/688 [==============================] - 0s 180us/step - loss: 0.1652 - accuracy: 0.9753\n",
      "Epoch 72/200\n",
      "688/688 [==============================] - 0s 177us/step - loss: 0.1613 - accuracy: 0.9767\n",
      "Epoch 73/200\n",
      "688/688 [==============================] - 0s 176us/step - loss: 0.1575 - accuracy: 0.9767\n",
      "Epoch 74/200\n",
      "688/688 [==============================] - 0s 203us/step - loss: 0.1539 - accuracy: 0.9767\n",
      "Epoch 75/200\n",
      "688/688 [==============================] - 0s 207us/step - loss: 0.1503 - accuracy: 0.9767\n",
      "Epoch 76/200\n",
      "688/688 [==============================] - 0s 178us/step - loss: 0.1470 - accuracy: 0.9767\n",
      "Epoch 77/200\n",
      "688/688 [==============================] - 0s 189us/step - loss: 0.1438 - accuracy: 0.9767\n",
      "Epoch 78/200\n",
      "688/688 [==============================] - 0s 223us/step - loss: 0.1406 - accuracy: 0.9767\n",
      "Epoch 79/200\n",
      "688/688 [==============================] - 0s 194us/step - loss: 0.1375 - accuracy: 0.9782\n",
      "Epoch 80/200\n",
      "688/688 [==============================] - 0s 184us/step - loss: 0.1346 - accuracy: 0.9782\n",
      "Epoch 81/200\n",
      "688/688 [==============================] - 0s 316us/step - loss: 0.1318 - accuracy: 0.9797\n",
      "Epoch 82/200\n",
      "688/688 [==============================] - 0s 201us/step - loss: 0.1290 - accuracy: 0.9797\n",
      "Epoch 83/200\n",
      "688/688 [==============================] - 0s 184us/step - loss: 0.1263 - accuracy: 0.9826\n",
      "Epoch 84/200\n",
      "688/688 [==============================] - 0s 169us/step - loss: 0.1237 - accuracy: 0.9840\n",
      "Epoch 85/200\n",
      "688/688 [==============================] - 0s 265us/step - loss: 0.1212 - accuracy: 0.9840\n",
      "Epoch 86/200\n",
      "688/688 [==============================] - 0s 228us/step - loss: 0.1187 - accuracy: 0.9840\n",
      "Epoch 87/200\n",
      "688/688 [==============================] - 0s 247us/step - loss: 0.1163 - accuracy: 0.9840\n",
      "Epoch 88/200\n",
      "688/688 [==============================] - 0s 212us/step - loss: 0.1140 - accuracy: 0.9855\n",
      "Epoch 89/200\n",
      "688/688 [==============================] - 0s 171us/step - loss: 0.1118 - accuracy: 0.9855\n",
      "Epoch 90/200\n",
      "688/688 [==============================] - 0s 228us/step - loss: 0.1098 - accuracy: 0.9855\n",
      "Epoch 91/200\n",
      "688/688 [==============================] - 0s 182us/step - loss: 0.1077 - accuracy: 0.9855\n",
      "Epoch 92/200\n",
      "688/688 [==============================] - 0s 184us/step - loss: 0.1057 - accuracy: 0.9855\n",
      "Epoch 93/200\n",
      "688/688 [==============================] - 0s 187us/step - loss: 0.1038 - accuracy: 0.9869\n",
      "Epoch 94/200\n",
      "688/688 [==============================] - 0s 182us/step - loss: 0.1019 - accuracy: 0.9869\n",
      "Epoch 95/200\n",
      "688/688 [==============================] - 0s 187us/step - loss: 0.0999 - accuracy: 0.9898\n",
      "Epoch 96/200\n",
      "688/688 [==============================] - 0s 174us/step - loss: 0.0982 - accuracy: 0.9898\n",
      "Epoch 97/200\n",
      "688/688 [==============================] - 0s 231us/step - loss: 0.0964 - accuracy: 0.9898\n",
      "Epoch 98/200\n",
      "688/688 [==============================] - 0s 195us/step - loss: 0.0948 - accuracy: 0.9898\n",
      "Epoch 99/200\n",
      "688/688 [==============================] - 0s 209us/step - loss: 0.0931 - accuracy: 0.9913\n",
      "Epoch 100/200\n",
      "688/688 [==============================] - 0s 171us/step - loss: 0.0914 - accuracy: 0.9913\n",
      "Epoch 101/200\n",
      "688/688 [==============================] - 0s 251us/step - loss: 0.0898 - accuracy: 0.9913\n",
      "Epoch 102/200\n",
      "688/688 [==============================] - 0s 256us/step - loss: 0.0883 - accuracy: 0.9913\n",
      "Epoch 103/200\n",
      "688/688 [==============================] - 0s 247us/step - loss: 0.0868 - accuracy: 0.9913\n",
      "Epoch 104/200\n",
      "688/688 [==============================] - 0s 242us/step - loss: 0.0854 - accuracy: 0.9913\n",
      "Epoch 105/200\n",
      "688/688 [==============================] - 0s 207us/step - loss: 0.0840 - accuracy: 0.9913\n",
      "Epoch 106/200\n",
      "688/688 [==============================] - 0s 181us/step - loss: 0.0827 - accuracy: 0.9913\n",
      "Epoch 107/200\n",
      "688/688 [==============================] - 0s 192us/step - loss: 0.0813 - accuracy: 0.9913\n",
      "Epoch 108/200\n",
      "688/688 [==============================] - 0s 213us/step - loss: 0.0801 - accuracy: 0.9913\n",
      "Epoch 109/200\n",
      "688/688 [==============================] - 0s 174us/step - loss: 0.0789 - accuracy: 0.9913\n",
      "Epoch 110/200\n",
      "688/688 [==============================] - 0s 253us/step - loss: 0.0777 - accuracy: 0.9913\n",
      "Epoch 111/200\n",
      "688/688 [==============================] - 0s 272us/step - loss: 0.0765 - accuracy: 0.9913\n",
      "Epoch 112/200\n",
      "688/688 [==============================] - 0s 220us/step - loss: 0.0754 - accuracy: 0.9913\n",
      "Epoch 113/200\n",
      "688/688 [==============================] - 0s 228us/step - loss: 0.0743 - accuracy: 0.9913\n",
      "Epoch 114/200\n",
      "688/688 [==============================] - 0s 248us/step - loss: 0.0732 - accuracy: 0.9913\n",
      "Epoch 115/200\n",
      "688/688 [==============================] - 0s 247us/step - loss: 0.0722 - accuracy: 0.9913\n",
      "Epoch 116/200\n",
      "688/688 [==============================] - 0s 299us/step - loss: 0.0712 - accuracy: 0.9913\n",
      "Epoch 117/200\n",
      "688/688 [==============================] - 0s 195us/step - loss: 0.0702 - accuracy: 0.9913\n",
      "Epoch 118/200\n",
      "688/688 [==============================] - 0s 188us/step - loss: 0.0692 - accuracy: 0.9913\n",
      "Epoch 119/200\n",
      "688/688 [==============================] - 0s 183us/step - loss: 0.0683 - accuracy: 0.9913\n",
      "Epoch 120/200\n",
      "688/688 [==============================] - 0s 211us/step - loss: 0.0673 - accuracy: 0.9913\n",
      "Epoch 121/200\n",
      "688/688 [==============================] - 0s 300us/step - loss: 0.0664 - accuracy: 0.9913\n",
      "Epoch 122/200\n",
      "688/688 [==============================] - 0s 381us/step - loss: 0.0656 - accuracy: 0.9913\n",
      "Epoch 123/200\n",
      "688/688 [==============================] - 0s 204us/step - loss: 0.0647 - accuracy: 0.9913\n",
      "Epoch 124/200\n",
      "688/688 [==============================] - 0s 282us/step - loss: 0.0639 - accuracy: 0.9913\n",
      "Epoch 125/200\n",
      "688/688 [==============================] - 0s 267us/step - loss: 0.0631 - accuracy: 0.9927\n",
      "Epoch 126/200\n",
      "688/688 [==============================] - 0s 220us/step - loss: 0.0622 - accuracy: 0.9927\n",
      "Epoch 127/200\n",
      "688/688 [==============================] - 0s 249us/step - loss: 0.0614 - accuracy: 0.9927\n",
      "Epoch 128/200\n",
      "688/688 [==============================] - 0s 375us/step - loss: 0.0607 - accuracy: 0.9927\n",
      "Epoch 129/200\n",
      "688/688 [==============================] - 0s 360us/step - loss: 0.0600 - accuracy: 0.9927\n",
      "Epoch 130/200\n",
      "688/688 [==============================] - 0s 412us/step - loss: 0.0592 - accuracy: 0.9927\n",
      "Epoch 131/200\n",
      "688/688 [==============================] - 0s 233us/step - loss: 0.0585 - accuracy: 0.9927\n",
      "Epoch 132/200\n",
      "688/688 [==============================] - 0s 215us/step - loss: 0.0578 - accuracy: 0.9927\n",
      "Epoch 133/200\n",
      "688/688 [==============================] - 0s 199us/step - loss: 0.0572 - accuracy: 0.9927\n",
      "Epoch 134/200\n",
      "688/688 [==============================] - 0s 348us/step - loss: 0.0565 - accuracy: 0.9927\n",
      "Epoch 135/200\n",
      "688/688 [==============================] - 0s 309us/step - loss: 0.0558 - accuracy: 0.9927\n",
      "Epoch 136/200\n",
      "688/688 [==============================] - 0s 253us/step - loss: 0.0552 - accuracy: 0.9927\n",
      "Epoch 137/200\n",
      "688/688 [==============================] - 0s 294us/step - loss: 0.0546 - accuracy: 0.9927\n",
      "Epoch 138/200\n",
      "688/688 [==============================] - 0s 398us/step - loss: 0.0540 - accuracy: 0.9927\n",
      "Epoch 139/200\n",
      "688/688 [==============================] - 0s 250us/step - loss: 0.0534 - accuracy: 0.9927\n",
      "Epoch 140/200\n",
      "688/688 [==============================] - 0s 238us/step - loss: 0.0528 - accuracy: 0.9927\n",
      "Epoch 141/200\n",
      "688/688 [==============================] - 0s 261us/step - loss: 0.0522 - accuracy: 0.9927\n",
      "Epoch 142/200\n",
      "688/688 [==============================] - 0s 518us/step - loss: 0.0516 - accuracy: 0.9927\n",
      "Epoch 143/200\n",
      "688/688 [==============================] - 0s 639us/step - loss: 0.0511 - accuracy: 0.9927\n",
      "Epoch 144/200\n",
      "688/688 [==============================] - 0s 348us/step - loss: 0.0506 - accuracy: 0.9927\n",
      "Epoch 145/200\n",
      "688/688 [==============================] - 0s 303us/step - loss: 0.0500 - accuracy: 0.9927\n",
      "Epoch 146/200\n",
      "688/688 [==============================] - 0s 411us/step - loss: 0.0496 - accuracy: 0.9927\n",
      "Epoch 147/200\n",
      "688/688 [==============================] - 0s 264us/step - loss: 0.0491 - accuracy: 0.9927\n",
      "Epoch 148/200\n",
      "688/688 [==============================] - 0s 324us/step - loss: 0.0486 - accuracy: 0.9927\n",
      "Epoch 149/200\n",
      "688/688 [==============================] - 0s 334us/step - loss: 0.0482 - accuracy: 0.9927\n",
      "Epoch 150/200\n",
      "688/688 [==============================] - 0s 427us/step - loss: 0.0477 - accuracy: 0.9927\n",
      "Epoch 151/200\n",
      "688/688 [==============================] - 0s 417us/step - loss: 0.0472 - accuracy: 0.9927\n",
      "Epoch 152/200\n",
      "688/688 [==============================] - 0s 272us/step - loss: 0.0468 - accuracy: 0.9927\n",
      "Epoch 153/200\n",
      "688/688 [==============================] - 0s 306us/step - loss: 0.0464 - accuracy: 0.9927\n",
      "Epoch 154/200\n",
      "688/688 [==============================] - 0s 228us/step - loss: 0.0459 - accuracy: 0.9927\n",
      "Epoch 155/200\n",
      "688/688 [==============================] - 0s 223us/step - loss: 0.0455 - accuracy: 0.9927\n",
      "Epoch 156/200\n",
      "688/688 [==============================] - 0s 317us/step - loss: 0.0451 - accuracy: 0.9927\n",
      "Epoch 157/200\n",
      "688/688 [==============================] - 0s 335us/step - loss: 0.0447 - accuracy: 0.9927\n",
      "Epoch 158/200\n",
      "688/688 [==============================] - 0s 329us/step - loss: 0.0443 - accuracy: 0.9927\n",
      "Epoch 159/200\n",
      "688/688 [==============================] - 0s 305us/step - loss: 0.0440 - accuracy: 0.9927\n",
      "Epoch 160/200\n",
      "688/688 [==============================] - 0s 311us/step - loss: 0.0436 - accuracy: 0.9927\n",
      "Epoch 161/200\n",
      "688/688 [==============================] - 0s 306us/step - loss: 0.0432 - accuracy: 0.9927\n",
      "Epoch 162/200\n",
      "688/688 [==============================] - 0s 325us/step - loss: 0.0428 - accuracy: 0.9927\n",
      "Epoch 163/200\n",
      "688/688 [==============================] - 0s 323us/step - loss: 0.0425 - accuracy: 0.9927\n",
      "Epoch 164/200\n",
      "688/688 [==============================] - 0s 291us/step - loss: 0.0421 - accuracy: 0.9927\n",
      "Epoch 165/200\n",
      "688/688 [==============================] - 0s 272us/step - loss: 0.0417 - accuracy: 0.9927\n",
      "Epoch 166/200\n",
      "688/688 [==============================] - 0s 238us/step - loss: 0.0414 - accuracy: 0.9927\n",
      "Epoch 167/200\n",
      "688/688 [==============================] - 0s 343us/step - loss: 0.0411 - accuracy: 0.9927\n",
      "Epoch 168/200\n",
      "688/688 [==============================] - 0s 262us/step - loss: 0.0408 - accuracy: 0.9927\n",
      "Epoch 169/200\n",
      "688/688 [==============================] - 0s 258us/step - loss: 0.0404 - accuracy: 0.9927\n",
      "Epoch 170/200\n",
      "688/688 [==============================] - 0s 242us/step - loss: 0.0401 - accuracy: 0.9927\n",
      "Epoch 171/200\n",
      "128/688 [====>.........................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9922"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, batch_size=128)\n",
    "#model.train_on_batch(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X2, y2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X2, batch_size=128)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = np.argmax(pred, axis=1)+1\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(pr, actualVal)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print (\"Accuracy: \" + str(accuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred:\n",
    "    count = sum(i)\n",
    "    for j in range(len(i)):\n",
    "        i[j] = i[j] / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "countGrade = copy.deepcopy(actualVal)\n",
    "countGrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(countGrade)):\n",
    "    ans = actualVal[i]\n",
    "    countGrade[i] = pred[i][ans-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sum(countGrade) / len(countGrade)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
